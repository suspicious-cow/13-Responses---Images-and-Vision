{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images and Vision\n",
    "\n",
    "## Basic Connection and Packages\n",
    "\n",
    "### Importing OpenAI and Initializing the Client\n",
    "\n",
    "To begin, we'll import the `OpenAI` class from the `openai` library, which allows us to interact with the OpenAI API. Next, we initialize a client instance, which we'll use to send requests and receive responses from the OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is a simple example of using the OpenAI API\n",
    "It uses the OpenAI Python client library to open a connection to the OpenAI API.\n",
    "This also looks for the OPENAI_API_KEY environment variable to authenticate the client.\n",
    "\"\"\"\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base64\n",
    "Next, we'll import Python's built-in `base64` library. This module allows us to encode or decode binary data (such as images or files) into a text-based representation, which is often required when working with images in API requests or responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing an Image URL\n",
    "In the following code cell, we'll use **\"gpt-4o\"** to analyze an image using a URL. The model will examine the picture and generate a descriptive response, which we'll then print out. This demonstrates how AI can interpret visual content alongside text-based instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/202412_Taiwan_Railway_Haifeng_EMU500_Tourist_Train_at_Houlong_Station.jpg\" width=\"512\" height=\"512\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a train at a station. The train has a distinct mint green color with two large windows in the front and several lights. There are also various details such as a nameplate, likely indicating the train's name or route. The surroundings include a fence next to the railway and some buildings in the background, with a clear sky above.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"Tell me what is in this image.\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/5/53/202412_Taiwan_Railway_Haifeng_EMU500_Tourist_Train_at_Houlong_Station.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a Base64 Encoded Image\n",
    "In the following code cell, we'll use **\"gpt-4o\"** to analyze an image using a URL. The model will examine the picture and generate a descriptive response, which we'll then print out. This demonstrates how AI can interpret visual content alongside text-based instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./artifacts/mystery_gathering.jpg\" width=\"512\" height=\"512\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a busy street scene with many people gathered. A person is seen jumping onto the roof of a parked black car, while others appear to be running or engaging in various activities around them. The environment looks urban, with buildings in the background and some construction or barriers visible. The overall atmosphere seems chaotic, as a large group is present, suggesting excitement or some sort of event.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to our image\n",
    "image_path = \"./artifacts/mystery_gathering.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"Tell me what is in this image.\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail: Low vs High Resolution\n",
    "The detail parameter tells the model what level of detail to use when processing and understanding the image (low, high, or auto to let the model decide). If you skip the parameter, the model will use auto.\n",
    "\n",
    "### Low Detail\n",
    "You can save tokens and speed up responses by using \"detail\": \"low\". This lets the model process the image with a budget of 85 tokens. The model receives a low-resolution 512px x 512px version of the image. This is fine if your use case doesn't require the model to see with high-resolution detail (for example, if you're asking about the dominant shape or color in the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A crowd of people is gathered on a city street, with one individual jumping on top of a parked car amidst the commotion.  \n",
      "2. A turquoise train awaits at a platform, surrounded by buildings and greenery under a soft, early morning light.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to our image\n",
    "image_path = \"./artifacts/mystery_gathering.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"Tell me all the details you see in this image.\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\":\"low\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Detail\n",
    "You can give the model more detail to generate its understanding by using \"detail\": \"high\". This lets the model see the low-resolution image (using 85 tokens) and then creates detailed crops using 170 tokens for each 512px x 512px tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears to be a busy urban street scene with multiple people gathered around a black sedan parked (or stopped) along the curb. Here’s a detailed, factual description of what can be observed:\n",
      "\n",
      "• Car in foreground:  \n",
      "  – A black four-door sedan with its rear license plate covered or edited out.  \n",
      "  – The roof and trunk area of the car appear intact, but a person is currently leaping onto it or over it.  \n",
      "\n",
      "• Person in red pants:  \n",
      "  – This individual is captured in midair above the car’s roof, wearing bright red pants, light-colored shoes, a gray or light-colored top, and a backpack with both shoulder straps on.  \n",
      "  – Their legs are bent upward, suggesting a jump or leap over (or possibly onto) the car.  \n",
      "\n",
      "• Surrounding crowd:  \n",
      "  – Many people, mostly on foot, are gathered in the street around the vehicle.  \n",
      "  – Several individuals seem to be in motion, with some gesturing or moving toward the car.  \n",
      "  – Their clothing varies, including T-shirts, sweatpants, and jackets in different colors such as white, black, pink, and gray.  \n",
      "  – At least one person is pushing or carrying what looks like a piece of equipment or furniture (it resembles a cart, speaker case, or small rolling container).  \n",
      "\n",
      "• Street context and background:  \n",
      "  – The urban street is lined with multi-story buildings, some constructed with brick facades and others with a more classic stone exterior.  \n",
      "  – Signs for “Cohen’s Optical” and “I Smoke Vape” (or a similar vape shop) are visible on the right side of the street.  \n",
      "  – A large construction or shipping container is on the left side, marked with lettering (though not fully legible).  \n",
      "  – Pedestrians, traffic barricades (orange in color), and an array of storefronts are visible.  \n",
      "  – Seemingly multiple stories of older-style architecture, along with more modern constructions in the background.  \n",
      "\n",
      "Overall, the image shows a moment of heightened activity, with one individual jumping and many onlookers or participants moving in various directions around a parked black sedan in the middle of a city street.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to our image\n",
    "image_path = \"./artifacts/mystery_gathering.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"Tell me all the details you see in this image.\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\":\"high\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Image Inputs\n",
    "The Responses API can take in and process multiple image inputs. The model processes each image and uses the information to answer questions about all images or each image independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to our image\n",
    "image_path = \"./artifacts/mystery_gathering.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"Give me one sentence describing each image you see.\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\":\"low\"\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/5/53/202412_Taiwan_Railway_Haifeng_EMU500_Tourist_Train_at_Houlong_Station.jpg\",\n",
    "            },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_api_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
